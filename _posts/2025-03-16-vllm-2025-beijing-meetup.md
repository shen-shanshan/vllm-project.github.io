---
layout: post
title: "vLLM Beijing meetup: Innovation, Ecosystem and Cmmunity"
author: "vLLM / vLLM Ascend / verl / LLaMAFactory Team"
image: /assets/logos/vllm-logo-text-light.png
---

On March 16, 2025, We hosted the tenth vLLM meetup with Huawei, the vLLM team, [verl](https://github.com/volcengine/verl), [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) team and [vLLM Ascend](https://github.com/vllm-project/vllm-ascend) team share how vLLM is leveraged in post-training, fine-tuning and deployment.

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/0.png" width="45%">
    </picture>
</p>

It is worth noting that this meetup marked the first vLLM community exchange event held in China. It not only provided a great platform for communication among major Chinese enterprises and universities but also strengthened the connection between the vLLM community and Chinese developers. In the future, we hope that through the joint efforts of the vLLM community and Chinese users, vLLM can be further refined, made more efficient, and user-friendly.

## Talks

### vLLM Update

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/1.png" width="45%">
    </picture>
</p>

Zhang Chen, one of the maintainers of vLLM, shared the recent work within the vLLM community and the release plan for the upcoming v0.8.0 version. She also highlighted the new features and usage methods of the vLLM V1 Engine. Compared to the V0 version, the V1 Engine will be more concise and efficient, and in the future, V1 will become the default option for vLLM.

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/2.png" width="45%">
    </picture>
</p>

You Kaichao, another maintainer of vLLM, shared the current ecosystem development status of the vLLM project in the industry and the communication channels of the vLLM community in China, including Zhihu and WeChat official accounts, which facilitate better connections between the vLLM team and Chinese developers.

### vLLM Hardware Plugin Mechanism and Ascend Best Practices

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/3.png" width="45%">
    </picture>
</p>

Wang Xiyuan, an engineer at Huawei and a core maintainer of the vllm-ascend project, shared Huawei's work on the vLLM hardware plugin mechanism using the Ascend NPU as an example. He explained the technology behind vLLM's easy implementation of multi-device backend support. Additionally, he introduced Huawei's Ascend AI chips and the principles and features of the CANN computing architecture, as well as Huawei's future plans for the vLLM community.

### VeRL: A Hybrid Controller-based RLHF Framework

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/4.png" width="45%">
    </picture>
</p>

Zhang Chi, an engineer at ByteDance and a core developer of the VeRL project, shared ByteDance's research and work in the field of reinforcement learning fine-tuning frameworks. He focused on the pain points currently addressed by VeRL and the core working principles of the Hybrid Controller.

### Best Practices for Efficient Fine-Tuning Framework LLaMA-Factory with vLLM

<p align="center">
    <picture>
    <img src="/assets/figures/vllm-2025-beijing-meetup/5.png" width="45%">
    </picture>
</p>

Zheng Yaowei, a researcher at Beihang University and a core maintainer of the LLaMA-Factory project, shared the current development status in the field of large model fine-tuning and the latest graphical interface launched by LLaMA-Factory. He also introduced how LLaMA-Factory works with frameworks like vLLM to provide developers with ultimate ease of use.
